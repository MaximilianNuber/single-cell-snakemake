# -----------------------------
# QC + Integration Snakefile
# -----------------------------
import os, csv
from pathlib import Path
try:
    from snakemake.exceptions import WorkflowError
except Exception:
    class WorkflowError(Exception): pass

# ---------- Samples from YAML dict or TSV ----------
def _read_samplesheet(path: Path):
    rows = []
    with open(path, newline="") as f:
        rdr = csv.DictReader(f, delimiter="\t")
        required = {"sample", "matrix"}
        if rdr.fieldnames is None or not required.issubset(rdr.fieldnames):
            missing = required - set(rdr.fieldnames or [])
            raise WorkflowError(f"Samples TSV missing columns: {sorted(missing)}")
        for row in rdr:
            s = (row.get("sample") or "").strip()
            m = (row.get("matrix") or "").strip()
            if s and m:
                rows.append({k: (v.strip() if isinstance(v, str) else v) for k, v in row.items()})
    base = path.parent.resolve()
    def resolve(p):
        p = Path(p)
        return str((base / p).resolve()) if not p.is_absolute() else str(p)
    sample_to_matrix, sample_meta = {}, {}
    for r in rows:
        if r["sample"] in sample_to_matrix:
            raise WorkflowError(f"Duplicate sample '{r['sample']}' in {path}")
        sample_to_matrix[r["sample"]] = resolve(r["matrix"])
        sample_meta[r["sample"]] = {k: v for k, v in r.items() if k not in ("sample", "matrix")}
    return sample_to_matrix, sample_meta

if "samples" in config:
    SAMPLE_TO_MATRIX = {str(k): str(v) for k, v in config["samples"].items()}
    SAMPLE_META = {k: {} for k in SAMPLE_TO_MATRIX}
elif "samples_tsv" in config:
    SHEET = Path(config["samples_tsv"]).resolve()
    SAMPLE_TO_MATRIX, SAMPLE_META = _read_samplesheet(SHEET)
else:
    default_sheet = Path("config/samples.tsv")
    if default_sheet.exists():
        SAMPLE_TO_MATRIX, SAMPLE_META = _read_samplesheet(default_sheet.resolve())
    else:
        raise WorkflowError("Provide either `samples:` (dict) or `samples_tsv:` in config.")

SAMPLES = sorted(SAMPLE_TO_MATRIX.keys())

# ---------- Integration config & env selection ----------
_INTEG  = config.get("integration", {})
_COMMON = _INTEG.get("common", {})
_RUNS   = _INTEG.get("runs", [{"name": "unintegrated", "method": "none"}])

RUNS   = [r["name"] for r in _RUNS]
RUNCFG = {r["name"]: ({**_COMMON, **r}) for r in _RUNS}
def rcfg(run_name): return RUNCFG[run_name]

_DEFAULT_ENVS = {
    "integrate": {
        "none":    "environments/scanpy.yaml",
        "bbknn":   "environments/scanpy.yaml",
        "harmony": "environments/harmony.yaml",
        "scvi":    "environments/scvi.yaml",
    },
    "graph": {
        "none":    "environments/scanpy.yaml",
        "bbknn":   "environments/bbknn.yaml",
        "harmony": "environments/scanpy.yaml",
        "scvi":    "environments/scanpy.yaml",
    },
}
def env_for(stage: str, method: str) -> str:
    m = (method or "none").lower()
    envs_cfg = config.get("integration", {}).get("envs", {})
    return envs_cfg.get(stage, {}).get(m, _DEFAULT_ENVS[stage][m])

# ---------- Make 'all' the FIRST rule (no wildcards) ----------
rule all:
    input:
        # QC products for all samples
        expand("results/qc/{sample}.metrics.h5ad", sample=SAMPLES),
        expand("results/qc/{sample}.filtered.h5ad", sample=SAMPLES),
        # concat + all integration variants
        "results/integration/concat.h5ad",
        expand("results/{run}/umap/umap.csv", run=RUNS),
        expand("results/{run}/cluster/leiden.csv", run=RUNS)

# -----------------------------
# QC
# -----------------------------
rule qc_metrics:
    input:
        matrix = lambda wc: SAMPLE_TO_MATRIX[wc.sample]
    output:
        h5ad = "results/qc/{sample}.metrics.h5ad"
    params:
        percent_top = lambda wc: config.get("qc", {}).get("percent_top", [20])
    conda:
        "environments/scanpy.yaml"
    script:
        "scripts/qc_compute_metrics.py"

DBL_METHOD = (config.get("tools", {}).get("doublets", {}).get("method", None))

if DBL_METHOD == "scdblfinder":
    rule doublets:
        input:  h5ad = "results/qc/{sample}.metrics.h5ad"
        output: tsv  = "results/qc/{sample}.doublets.tsv"
        params: batch_key = lambda wc: config["tools"]["doublets"]["params"].get("batch_key", None)
        conda:  "environments/r_qc.yaml"
        script: "scripts/run_scdblfinder.py"

elif DBL_METHOD == "scrublet":
    rule doublets:
        input:  h5ad = "results/qc/{sample}.metrics.h5ad"
        output: tsv  = "results/qc/{sample}.doublets.tsv"
        params: scrublet_params = lambda wc: config["tools"]["doublets"]["params"]
        conda:  "environments/scrublet_legacy.yaml"
        script: "scripts/run_scrublet.py"

else:
    rule doublets:
        input:  h5ad = "results/qc/{sample}.metrics.h5ad"
        output: tsv  = "results/qc/{sample}.doublets.tsv"
        run:
            os.makedirs(os.path.dirname(output.tsv), exist_ok=True)
            with open(output.tsv, "w") as f:
                f.write("barcode\tis_doublet\tdbl_score\n")

rule qc_filter:
    input:
        h5ad = "results/qc/{sample}.metrics.h5ad",
        dbl  = "results/qc/{sample}.doublets.tsv"
    output:
        h5ad = "results/qc/{sample}.filtered.h5ad"
    params:
        qc_cfg = lambda wc: config.get("qc", {}),
        percent_top = lambda wc: (config.get("qc", {}).get("percent_top", [20]) or [20])[0]
    conda:
        "environments/scanpy.yaml"
    script:
        "scripts/qc_filter.py"

# -----------------------------
# Integration
# -----------------------------
rule concat_samples:
    input:
        expand("results/qc/{s}.filtered.h5ad", s=SAMPLES)
    output:
        "results/integration/concat.h5ad"
    params:
        batch_key = lambda wc: _COMMON.get("batch_key", "sample")
    conda:
        "environments/scanpy.yaml"
    script:
        "scripts/concat_samples.py"

# rule pca:
#     input:
#         "results/integration/concat.h5ad"
#     output:
#         directory("results/integration/pca")
#     params:
#         pca_cfg = {
#             "target_sum": 1e4,
#             "log1p": True,
#             "n_hvgs": _COMMON.get("n_top_genes", 3000),
#             "hvg_flavor": "seurat_v3",
#             "scale_max": 10.0,
#             "n_comps": _COMMON.get("pca_comps", 50),
#             "zero_center": True,
#             "svd_solver": "arpack",
#             "random_state": 0,
#         }
#     conda:
#         "environments/scanpy.yaml"
#     script:
#         "scripts/pca_from_concat.py"
rule pca:
    input:
        "results/integration/concat.h5ad"
    output:
        x        = "results/integration/pca/X_pca.npy",
        idx      = "results/integration/pca/X_pca.index.csv",
        manifest = "results/integration/pca/manifest.json"
    params:
        pca_cfg = {
            "target_sum": 1e4,
            "log1p": True,
            "n_hvgs": _COMMON.get("n_top_genes", 3000),
            "hvg_flavor": "seurat_v3",
            "scale_max": 10.0,
            "n_comps": _COMMON.get("pca_comps", 50),
            "zero_center": True,
            "svd_solver": "arpack",
            "random_state": 0,
        }
    conda:
        "environments/scanpy.yaml"
    script:
        "scripts/pca_from_concat.py"

rule integrate_run:
    input:
        concat = "results/integration/concat.h5ad",
        pca    = "results/integration/pca/X_pca.npy"
    output:
        emb_dir  = directory("results/{run}/embedding"),
        manifest = "results/{run}/embedding/manifest.json"
    params:
        run_cfg = lambda wc: rcfg(wc.run)
    conda:
        lambda wc: env_for("integrate", rcfg(wc.run).get("method"))
    script:
        "scripts/integrate_toggle.py"

rule graph_layout_cluster_run:
    input:
        concat   = "results/integration/concat.h5ad",
        latent   = "results/{run}/embedding/X_latent.npy",
        idx      = "results/{run}/embedding/X_latent.index.csv",
        manifest = "results/{run}/embedding/manifest.json"
    output:
        knn      = "results/{run}/graph/knn.npz",
        umap     = "results/{run}/umap/umap.csv",
        leiden   = "results/{run}/cluster/leiden.csv",
        manifest = "results/{run}/graph/manifest.json"
    params:
        run_cfg = lambda wc: rcfg(wc.run)
    conda:
        lambda wc: env_for("graph", rcfg(wc.run).get("method"))
    script:
        "scripts/graph_umap_leiden.py"
